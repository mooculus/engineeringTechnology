\documentclass{ximera}

\input{../../preamble.tex}
\input{../../sage-preamble.tex}

\author{Jim Fowler \and Bart Snapp}

\outcome{State the definition of the divergence of a vector field in any dimension.}

\begin{document}
\makerandom
\begin{sagesilent}
  vs = ['x','y']
  
  x = var(vs[0])
  y = var(vs[1])

  def random_expression(depth):
    if depth == 0:
      return [x,x,x,y,y,y,1,-1][randint(0,7)]
    else:
      possible = [
        lambda: random_expression(depth-1) + random_expression(depth-1),
        lambda: random_expression(depth-1) - random_expression(depth-1),
        lambda: random_expression(depth-1) * random_expression(depth-1),
        lambda: random_expression(depth-1) / random_expression(depth-1),
        lambda: sin(random_expression(depth-1)),
        lambda: cos(random_expression(depth-1)),
        lambda: log(random_expression(depth-1)),
        lambda: exp(random_expression(depth-1))
      ]
      result = possible[randint(0,len(possible)-1)]()
      if result.is_zero():
        result = 1
      return result

  m = random_expression(3)
  while (derivative(m,x).is_zero()) or (len(latex(derivative(m,x))) > 40) or (len(m.variables()) < 2):
    m = random_expression(randint(1,3))

  n = random_expression(3)
  while (derivative(n,y).is_zero()) or (len(latex(derivative(n,y))) > 40) or (len(n.variables()) < 2):
    n = random_expression(randint(1,3))
  
\end{sagesilent}

\begin{exercise}

  Let $\vec{F}(x,y) = \vector{\sage{m},\sage{n}}$, compute $\divergence \vec{F}$.
  \begin{prompt}
  \[
  \divergence \vec{F}(x,y) = \answer{\sage{derivative(m,x) + derivative(n,y)}}
  \]
  \end{prompt}
  \begin{hint}
    Note, in all advanced mathematics courses, including this one, $\log(x) = \ln(x)$.
  \end{hint}
\end{exercise}
\end{document}
